{
    "experiment": {
       "id": "exp2.0-visualbert-baseline-2015",
       "description": "Fine-tuning VisualBERT on Twitter-2015 dataset",
       "do_training": true,
       "output_dir": "results",
       "input_ckpt_path": "results/exp2.0-visualbert-baseline-2015/checkpoint/",
       "output_ckpt": "results/exp2.0-visualbert-baseline-2015/checkpoint/model.pt",
       "num_epoch_checkpoints": 5
    },
    "data": {
        "directory": "../datas/twitter2015",
        "dataset_class": "mner",
        "text": {
            "train": "text/train.txt",
            "valid": "text/valid.txt",
            "test": "text/test.txt"
        },
        "image":{
            "embedding_dim": 2048,
            "size": null,
            "train": "twitter2015_images",
            "valid": "twitter2015_images",
            "test": "twitter2015_images"
        },
        "caption":{
            "train": null,
            "valid": null,
            "test": null
        },
        "processed_data_path": {
            "train": "processed_multi_train_data.pt",
            "valid": "processed_multi_valid_data.pt",
            "test": "processed_multi_test_data.pt"
        },
        "label_scheme": ["B-LOC", "B-OTHER", "B-ORG", "B-PER", "I-LOC", "I-OTHER", "I-ORG", "I-PER", "O"]
    },
    "model": {
        "name": "mner_prompt",
        "model_name_or_path": "bert-base-uncased",
        "pretrained_weights": "./pretrain_bert/bert-base-uncased/pytorch_model.bin",
        "do_lower_case": true,
        "output_attentions": false,
        "output_hidden_states": false,
        "pre_seq_len": 40,
        "att_hidden_size": 768,
        "dim_feedforward": 2048,
        "num_head": 12,
        "hidden_dropout_prob": 0.4
    },
    "training": {
        "epochs": 50,
        "patience": 10,
        "per_gpu_train_batch_size": 16,
        "per_gpu_eval_batch_size": 16,
        "optim": {
            "learning_rate": 5e-5,
            "max_steps": -1,
            "gradient_accumulation_steps": 1,
            "weight_decay": 0.01,
            "beta_1": 0.9,
            "beta_2": 0.999,
            "adam_epsilon": 1e-8,
            "max_grad_norm": 1.0,
            "warmup_steps": 0
        }
    }
}